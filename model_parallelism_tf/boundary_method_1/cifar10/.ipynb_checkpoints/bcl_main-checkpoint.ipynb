{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcl\n",
    "import bcl_model\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10\n",
    "import cifar10_model\n",
    "import cifar10_utils\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fn(num_gpus, variable_strategy, num_workers):\n",
    "    \"\"\"Returns a function that will build the resnet model.\"\"\"\n",
    "\n",
    "    def _bcl_model_fn(features, labels, mode, params):\n",
    "        \"\"\"Resnet model body.\n",
    "        Support single host, one or more GPU training. Parameter distribution can\n",
    "        be either one of the following scheme.\n",
    "        1. CPU is the parameter server and manages gradient updates.\n",
    "        2. Parameters are distributed evenly across all GPUs, and the first GPU\n",
    "           manages gradient updates.\n",
    "        Args:\n",
    "          features: a list of tensors, one for each tower\n",
    "          labels: a list of tensors, one for each tower\n",
    "          mode: ModeKeys.TRAIN or EVAL\n",
    "          params: Hyperparameters suitable for tuning\n",
    "        Returns:\n",
    "          A EstimatorSpec object.\n",
    "        \"\"\"\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        weight_decay = params.weight_decay\n",
    "        momentum = params.momentum\n",
    "\n",
    "        tower_inputs = features\n",
    "        tower_groundtruths = labels\n",
    "        tower_losses = []\n",
    "        tower_gradvars = []\n",
    "        tower_preds = []\n",
    "\n",
    "        # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n",
    "        # on CPU. The exception is Intel MKL on CPU which is optimal with\n",
    "        # channels_last.\n",
    "        data_format = params.data_format\n",
    "        if not data_format:\n",
    "            if num_gpus == 0:\n",
    "                data_format = 'channels_last'\n",
    "            else:\n",
    "                data_format = 'channels_first'\n",
    "\n",
    "        if num_gpus == 0:\n",
    "            num_devices = 1\n",
    "            device_type = 'cpu'\n",
    "        else:\n",
    "            num_devices = num_gpus\n",
    "            device_type = 'gpu'\n",
    "\n",
    "        for i in range(num_devices):\n",
    "            worker_device = '/{}:{}'.format(device_type, i)\n",
    "            if variable_strategy == 'CPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    worker_device=worker_device)\n",
    "            elif variable_strategy == 'GPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    ps_device_type='gpu',\n",
    "                    worker_device=worker_device,\n",
    "                    ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n",
    "                        num_gpus, tf.contrib.training.byte_size_load_fn))\n",
    "            with tf.variable_scope('bcl', reuse=bool(i != 0)):\n",
    "                with tf.name_scope('tower_%d' % i) as name_scope:\n",
    "                    with tf.device(device_setter):\n",
    "                        loss, gradvars, preds = _tower_fn(\n",
    "                            is_training, weight_decay, tower_inputs[i], tower_groundtruths[i],\n",
    "                            data_format, params.num_layers, params.batch_norm_decay,\n",
    "                            params.batch_norm_epsilon)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_gradvars.append(gradvars)\n",
    "                        tower_preds.append(preds)\n",
    "                        if i == 0:\n",
    "                            # Only trigger batch_norm moving mean and variance update from\n",
    "                            # the 1st tower. Ideally, we should grab the updates from all\n",
    "                            # towers but these stats accumulate extremely fast so we can\n",
    "                            # ignore the other stats from the other towers without\n",
    "                            # significant detriment.\n",
    "                            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n",
    "                                                           name_scope)\n",
    "\n",
    "        # Now compute global loss and gradients.\n",
    "        gradvars = []\n",
    "        with tf.name_scope('gradient_averaging'):\n",
    "            all_grads = {}\n",
    "            for grad, var in itertools.chain(*tower_gradvars):\n",
    "                if grad is not None:\n",
    "                    all_grads.setdefault(var, []).append(grad)\n",
    "            for var, grads in six.iteritems(all_grads):\n",
    "                # Average gradients on the same device as the variables\n",
    "                # to which they apply.\n",
    "                with tf.device(var.device):\n",
    "                    if len(grads) == 1:\n",
    "                        avg_grad = grads[0]\n",
    "                    else:\n",
    "                        avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n",
    "                gradvars.append((avg_grad, var))\n",
    "\n",
    "        # Device that runs the ops to apply global gradient updates.\n",
    "        consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'\n",
    "        with tf.device(consolidation_device):\n",
    "            # Suggested learning rate scheduling from\n",
    "            # https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/cifar10-resnet.py#L155\n",
    "            num_batches_per_epoch = cifar10.Cifar10DataSet.num_examples_per_epoch(\n",
    "                'train') // (params.train_batch_size * num_workers)\n",
    "            boundaries = [\n",
    "                num_batches_per_epoch * x\n",
    "                for x in np.array([82, 123, 300], dtype=np.int64)\n",
    "            ]\n",
    "            staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "\n",
    "            learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n",
    "                                                        boundaries, staged_lr)\n",
    "\n",
    "            loss = tf.reduce_mean(tower_losses, name='loss')\n",
    "\n",
    "            examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(\n",
    "                params.train_batch_size, every_n_steps=10)\n",
    "\n",
    "            tensors_to_log = {'learning_rate': learning_rate, 'loss': loss}\n",
    "\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                tensors=tensors_to_log, every_n_iter=100)\n",
    "\n",
    "            train_hooks = [logging_hook, examples_sec_hook]\n",
    "\n",
    "            optimizer = tf.train.MomentumOptimizer(\n",
    "                learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "            if params.sync:\n",
    "                optimizer = tf.train.SyncReplicasOptimizer(\n",
    "                    optimizer, replicas_to_aggregate=num_workers)\n",
    "                sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n",
    "                train_hooks.append(sync_replicas_hook)\n",
    "\n",
    "            # Create single grouped train op\n",
    "            train_op = [\n",
    "                optimizer.apply_gradients(\n",
    "                    gradvars, global_step=tf.train.get_global_step())\n",
    "            ]\n",
    "            train_op.extend(update_ops)\n",
    "            train_op = tf.group(*train_op)\n",
    "\n",
    "            predictions = tf.concat(tower_preds, axis=0)\n",
    "            groundtruths = tf.concat(groundtruths, axis=0)\n",
    "            metrics = {\n",
    "                'mse':\n",
    "                    tf.metrics.mean_squared_error(groundtruths, predictions)\n",
    "            }\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            training_hooks=train_hooks,\n",
    "            eval_metric_ops=metrics)\n",
    "\n",
    "    return _bcl_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tower_fn(is_training, weight_decay, inputs, groundtruths, data_format,\n",
    "              num_layers, batch_norm_decay, batch_norm_epsilon):\n",
    "    \"\"\"Build computation tower (Resnet).\n",
    "    Args:\n",
    "    is_training: true if is training graph.\n",
    "    weight_decay: weight regularization strength, a float.\n",
    "    feature: a Tensor.\n",
    "    label: a Tensor.\n",
    "    data_format: channels_last (NHWC) or channels_first (NCHW).\n",
    "    num_layers: number of layers, an int.\n",
    "    batch_norm_decay: decay for batch normalization, a float.\n",
    "    batch_norm_epsilon: epsilon for batch normalization, a float.\n",
    "    Returns:\n",
    "    A tuple with the loss for the tower, the gradients and parameters, and\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    model = bcl_model.BCL(\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=data_format)\n",
    "    #tower_pred = model.forward_pass(inputs, input_data_format='channels_last')\n",
    "    tower_pred = model.forward_pass(inputs)\n",
    "\n",
    "    tower_loss = tf.losses.mean_squared_error(\n",
    "        labels=groundtruths, predictions=tower_pred)\n",
    "    tower_loss = tf.reduce_mean(tower_loss)\n",
    "\n",
    "    model_params = tf.trainable_variables()\n",
    "    tower_loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "\n",
    "    tower_grad = tf.gradients(tower_loss, model_params)\n",
    "\n",
    "    return tower_loss, zip(tower_grad, model_params), tower_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_dir,\n",
    "             subset,\n",
    "             num_shards,\n",
    "             batch_size,\n",
    "             use_distortion_for_training=True):\n",
    "    \"\"\"Create input graph for model.\n",
    "        Args:\n",
    "        data_dir: Directory where TFRecords representing the dataset are located.\n",
    "        subset: one of 'train', 'validate' and 'eval'.\n",
    "        num_shards: num of towers participating in data-parallel training.\n",
    "        batch_size: total batch size for training to be divided by the number of\n",
    "        shards.\n",
    "        use_distortion_for_training: True to use distortions.\n",
    "        Returns:\n",
    "        two lists of tensors for features and labels, each of num_shards length.\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        use_distortion = subset == 'train' and use_distortion_for_training\n",
    "        dataset = bcl.BCLDataSet(data_dir, subset, use_distortion)\n",
    "        image_batch, label_batch = dataset.make_batch(batch_size)\n",
    "        if num_shards <= 1:\n",
    "            # No GPU available or only 1 GPU.\n",
    "            return [image_batch], [label_batch]\n",
    "\n",
    "        # Note that passing num=batch_size is safe here, even though\n",
    "        # dataset.batch(batch_size) can, in some cases, return fewer than batch_size\n",
    "        # examples. This is because it does so only when repeating for a limited\n",
    "        # number of epochs, but our dataset repeats forever.\n",
    "        image_batch = tf.unstack(image_batch, num=batch_size, axis=0)\n",
    "        label_batch = tf.unstack(label_batch, num=batch_size, axis=0)\n",
    "        image_shards = [[] for i in range(num_shards)]\n",
    "        label_shards = [[] for i in range(num_shards)]\n",
    "        for i in range(batch_size):\n",
    "            idx = i % num_shards\n",
    "            image_shards[idx].append(image_batch[i])\n",
    "            label_shards[idx].append(label_batch[i])\n",
    "        image_shards = [tf.parallel_stack(x) for x in image_shards]\n",
    "        label_shards = [tf.parallel_stack(x) for x in label_shards]\n",
    "        return image_shards, label_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_fn(data_dir,\n",
    "                      num_gpus,\n",
    "                      variable_strategy,\n",
    "                      use_distortion_for_training=True):\n",
    "    \"\"\"Returns an Experiment function.\n",
    "    Experiments perform training on several workers in parallel,\n",
    "    in other words experiments know how to invoke train and eval in a sensible\n",
    "    fashion for distributed training. Arguments passed directly to this\n",
    "    function are not tunable, all other arguments should be passed within\n",
    "    tf.HParams, passed to the enclosed function.\n",
    "    Args:\n",
    "      data_dir: str. Location of the data for input_fns.\n",
    "      num_gpus: int. Number of GPUs on each worker.\n",
    "      variable_strategy: String. CPU to use CPU as the parameter server\n",
    "      and GPU to use the GPUs as the parameter server.\n",
    "      use_distortion_for_training: bool. See cifar10.Cifar10DataSet.\n",
    "    Returns:\n",
    "      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n",
    "      tf.contrib.learn.Experiment.\n",
    "      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n",
    "      methods on Experiment (train, evaluate) based on information\n",
    "      about the current runner in `run_config`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _experiment_fn(run_config, hparams):\n",
    "        \"\"\"Returns an Experiment.\"\"\"\n",
    "        # Create estimator.\n",
    "        train_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='train',\n",
    "            num_shards=num_gpus,\n",
    "            batch_size=hparams.train_batch_size,\n",
    "            use_distortion_for_training=use_distortion_for_training)\n",
    "\n",
    "        val_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='val',\n",
    "            batch_size=hparams.val_batch_size,\n",
    "            num_shards=num_gpus)\n",
    "\n",
    "        num_val_examples = bcl.BCLDataSet.num_examples_per_epoch('val')\n",
    "\n",
    "        train_steps = hparams.train_steps\n",
    "        val_steps = num_val_examples // hparams.val_batch_size\n",
    "\n",
    "        classifier = tf.estimator.Estimator(\n",
    "            model_fn=get_model_fn(num_gpus, variable_strategy,\n",
    "                                  run_config.num_worker_replicas or 1),\n",
    "            config=run_config,\n",
    "            params=hparams)\n",
    "\n",
    "        # Create experiment.\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            classifier,\n",
    "            train_input_fn=train_input_fn,\n",
    "            eval_input_fn=val_input_fn,\n",
    "            train_steps=train_steps,\n",
    "            eval_steps=val_steps)\n",
    "\n",
    "    return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(job_dir, data_dir, num_gpus, variable_strategy,\n",
    "         use_distortion_for_training, log_device_placement, num_intra_threads,\n",
    "         **hparams):\n",
    "    # The env variable is on deprecation path, default is set to off.\n",
    "    os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "    # Session configuration.\n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=log_device_placement,\n",
    "        intra_op_parallelism_threads=num_intra_threads,\n",
    "        gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n",
    "\n",
    "    config = cifar10_utils.RunConfig(\n",
    "        session_config=sess_config, model_dir=job_dir)\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        get_experiment_fn(data_dir, num_gpus, variable_strategy,\n",
    "                          use_distortion_for_training),\n",
    "        run_config=config,\n",
    "        hparams=tf.contrib.training.HParams(\n",
    "            is_chief=config.is_chief,\n",
    "            **hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"../sequence_data/12\",\n",
    "    \"job_dir\": \"tmp/bcl\",\n",
    "    \"variable_strategy\": \"CPU\",\n",
    "    \"num_gpus\": 2,\n",
    "    \"num_layers\": 4,\n",
    "    \"train_steps\": 2,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 32,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 2e-4,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"use_distortion_for_training\": False,\n",
    "    \"sync\": False,\n",
    "    \"num_intra_threads\": 0,\n",
    "    \"num_inter_threads\": 0,\n",
    "    \"data_format\": \"channels_last\",\n",
    "    \"log_device_placement\": False,\n",
    "    \"batch_norm_decay\": 0.997,\n",
    "    \"batch_norm_epsilon\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f03b036bac8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'tmp/bcl'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "is_training: True\n",
      "is_training: True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HParams' object has no attribute 'sync'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f8e639b89338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-f8e6aef44f71>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(job_dir, data_dir, num_gpus, variable_strategy, use_distortion_for_training, log_device_placement, num_intra_threads, **hparams)\u001b[0m\n\u001b[1;32m     21\u001b[0m         hparams=tf.contrib.training.HParams(\n\u001b[1;32m     22\u001b[0m             \u001b[0mis_chief\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_chief\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             **hparams))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               instructions)\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(experiment_fn, output_dir, schedule, run_config, hparams)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0mschedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_default_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_execute_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\u001b[0m in \u001b[0;36m_execute_schedule\u001b[0;34m(experiment, schedule)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Allowed values for this experiment are: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Schedule references non-callable member %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m                   hooks=self._eval_hooks)\n\u001b[1;32m    671\u001b[0m           ]\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;31m# If the checkpoint_and_export flag and appropriate estimator configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, delay_secs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_monitors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         saving_listeners=self._saving_listeners)\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36m_call_train\u001b[0;34m(self, _sentinel, input_fn, steps, hooks, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    881\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    884\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       return self._estimator.fit(\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1235\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1237\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1238\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/anaconda3/envs/pytf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f75dac1e9a2f>\u001b[0m in \u001b[0;36m_bcl_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 learning_rate=learning_rate, momentum=momentum)\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 optimizer = tf.train.SyncReplicasOptimizer(\n\u001b[1;32m    126\u001b[0m                     optimizer, replicas_to_aggregate=num_workers)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HParams' object has no attribute 'sync'"
     ]
    }
   ],
   "source": [
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
