{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcl\n",
    "import bcl_model\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"0,1,2,3\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "used_gpus = [0, 1, 2, 3]\n",
    "s = str(used_gpus[0])\n",
    "for used_gpu in used_gpus[1:]:\n",
    "    s += ',{}'.format(used_gpu)\n",
    "\n",
    "s1 = \"\\\"{}\\\"\".format(s)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10_utils\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fn(num_gpus, variable_strategy, num_workers):\n",
    "    \"\"\"Returns a function that will build the resnet model.\"\"\"\n",
    "\n",
    "    def _bcl_model_fn(features, labels, mode, params):\n",
    "        \"\"\"Resnet model body.\n",
    "        Support single host, one or more GPU training. Parameter distribution can\n",
    "        be either one of the following scheme.\n",
    "        1. CPU is the parameter server and manages gradient updates.\n",
    "        2. Parameters are distributed evenly across all GPUs, and the first GPU\n",
    "           manages gradient updates.\n",
    "        Args:\n",
    "          features: a list of tensors, one for each tower\n",
    "          labels: a list of tensors, one for each tower\n",
    "          mode: ModeKeys.TRAIN or EVAL\n",
    "          params: Hyperparameters suitable for tuning\n",
    "        Returns:\n",
    "          A EstimatorSpec object.\n",
    "        \"\"\"\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        weight_decay = params.weight_decay\n",
    "        momentum = params.momentum\n",
    "\n",
    "        tower_inputs = features\n",
    "        tower_groundtruths = labels\n",
    "        tower_losses = []\n",
    "        tower_gradvars = []\n",
    "        tower_preds = []\n",
    "\n",
    "        # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n",
    "        # on CPU. The exception is Intel MKL on CPU which is optimal with\n",
    "        # channels_last.\n",
    "        data_format = params.data_format\n",
    "        if not data_format:\n",
    "            if num_gpus == 0:\n",
    "                data_format = 'channels_last'\n",
    "            else:\n",
    "                data_format = 'channels_first'\n",
    "\n",
    "        if num_gpus == 0:\n",
    "            num_devices = 1\n",
    "            device_type = 'cpu'\n",
    "        else:\n",
    "            num_devices = num_gpus\n",
    "            device_type = 'gpu'\n",
    "\n",
    "        for i in range(num_devices):\n",
    "            worker_device = '/{}:{}'.format(device_type, i)\n",
    "            if variable_strategy == 'CPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    worker_device=worker_device)\n",
    "            elif variable_strategy == 'GPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    ps_device_type='gpu',\n",
    "                    worker_device=worker_device,\n",
    "                    ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n",
    "                        num_gpus, tf.contrib.training.byte_size_load_fn))\n",
    "            with tf.variable_scope('bcl', reuse=bool(i != 0)):\n",
    "                with tf.name_scope('tower_%d' % i) as name_scope:\n",
    "                    with tf.device(device_setter):\n",
    "                        loss, gradvars, preds = _tower_fn(\n",
    "                            is_training, weight_decay, tower_inputs[i], tower_groundtruths[i],\n",
    "                            data_format, params.num_layers, params.batch_norm_decay,\n",
    "                            params.batch_norm_epsilon)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_gradvars.append(gradvars)\n",
    "                        tower_preds.append(preds)\n",
    "                        if i == 0:\n",
    "                            # Only trigger batch_norm moving mean and variance update from\n",
    "                            # the 1st tower. Ideally, we should grab the updates from all\n",
    "                            # towers but these stats accumulate extremely fast so we can\n",
    "                            # ignore the other stats from the other towers without\n",
    "                            # significant detriment.\n",
    "                            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n",
    "                                                           name_scope)\n",
    "\n",
    "        # Now compute global loss and gradients.\n",
    "        gradvars = []\n",
    "        with tf.name_scope('gradient_averaging'):\n",
    "            all_grads = {}\n",
    "            for grad, var in itertools.chain(*tower_gradvars):\n",
    "                if grad is not None:\n",
    "                    all_grads.setdefault(var, []).append(grad)\n",
    "            for var, grads in six.iteritems(all_grads):\n",
    "                # Average gradients on the same device as the variables\n",
    "                # to which they apply.\n",
    "                with tf.device(var.device):\n",
    "                    if len(grads) == 1:\n",
    "                        avg_grad = grads[0]\n",
    "                    else:\n",
    "                        avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n",
    "                gradvars.append((avg_grad, var))\n",
    "\n",
    "        # Device that runs the ops to apply global gradient updates.\n",
    "        consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'\n",
    "        with tf.device(consolidation_device):\n",
    "            # Suggested learning rate scheduling from\n",
    "            # https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/cifar10-resnet.py#L155\n",
    "            num_batches_per_epoch = bcl.BCLDataSet.num_examples_per_epoch(\n",
    "                'train') // (params.train_batch_size * num_workers)\n",
    "            boundaries = [\n",
    "                num_batches_per_epoch * x\n",
    "                for x in np.array([82, 123, 300], dtype=np.int64)\n",
    "            ]\n",
    "            staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "\n",
    "            learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n",
    "                                                        boundaries, staged_lr)\n",
    "\n",
    "            loss = tf.reduce_mean(tower_losses, name='loss')\n",
    "\n",
    "            examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(\n",
    "                params.train_batch_size, every_n_steps=10)\n",
    "            \n",
    "            optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate=params.learning_rate, momentum=momentum)\n",
    "\n",
    "            tensors_to_log = {'loss': loss}\n",
    "\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                tensors=tensors_to_log, every_n_iter=100)\n",
    "\n",
    "            train_hooks = [logging_hook, examples_sec_hook]\n",
    "\n",
    "            if params.sync:\n",
    "                optimizer = tf.train.SyncReplicasOptimizer(\n",
    "                    optimizer, replicas_to_aggregate=num_workers)\n",
    "                sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n",
    "                train_hooks.append(sync_replicas_hook)\n",
    "\n",
    "            # Create single grouped train op\n",
    "            train_op = [\n",
    "                optimizer.apply_gradients(\n",
    "                    gradvars, global_step=tf.train.get_global_step())\n",
    "            ]\n",
    "            train_op.extend(update_ops)\n",
    "            train_op = tf.group(*train_op)\n",
    "\n",
    "            predictions = tf.concat(tower_preds, axis=0)\n",
    "            groundtruths = tf.concat(labels, axis=0)\n",
    "            metrics = {\n",
    "                'mse':\n",
    "                    tf.metrics.mean_squared_error(groundtruths, predictions)\n",
    "            }\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            training_hooks=train_hooks,\n",
    "            eval_metric_ops=metrics)\n",
    "\n",
    "    return _bcl_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tower_fn(is_training, weight_decay, inputs, groundtruths, data_format,\n",
    "              num_layers, batch_norm_decay, batch_norm_epsilon):\n",
    "    \"\"\"Build computation tower (Resnet).\n",
    "    Args:\n",
    "    is_training: true if is training graph.\n",
    "    weight_decay: weight regularization strength, a float.\n",
    "    feature: a Tensor.\n",
    "    label: a Tensor.\n",
    "    data_format: channels_last (NHWC) or channels_first (NCHW).\n",
    "    num_layers: number of layers, an int.\n",
    "    batch_norm_decay: decay for batch normalization, a float.\n",
    "    batch_norm_epsilon: epsilon for batch normalization, a float.\n",
    "    Returns:\n",
    "    A tuple with the loss for the tower, the gradients and parameters, and\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    model = bcl_model.BCL(\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=data_format)\n",
    "    #tower_pred = model.forward_pass(inputs, input_data_format='channels_last')\n",
    "    tower_pred = model.forward_pass(inputs)\n",
    "\n",
    "    tower_loss = tf.losses.mean_squared_error(\n",
    "        labels=groundtruths, predictions=tower_pred)\n",
    "    tower_loss = tf.reduce_mean(tower_loss)\n",
    "\n",
    "    model_params = tf.trainable_variables()\n",
    "    #tower_loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "\n",
    "    tower_grad = tf.gradients(tower_loss, model_params)\n",
    "\n",
    "    return tower_loss, zip(tower_grad, model_params), tower_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_dir,\n",
    "             subset,\n",
    "             num_shards,\n",
    "             batch_size,\n",
    "             use_distortion_for_training=True):\n",
    "    \"\"\"Create input graph for model.\n",
    "        Args:\n",
    "        data_dir: Directory where TFRecords representing the dataset are located.\n",
    "        subset: one of 'train', 'validate' and 'eval'.\n",
    "        num_shards: num of towers participating in data-parallel training.\n",
    "        batch_size: total batch size for training to be divided by the number of\n",
    "        shards.\n",
    "        use_distortion_for_training: True to use distortions.\n",
    "        Returns:\n",
    "        two lists of tensors for features and labels, each of num_shards length.\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        use_distortion = subset == 'train' and use_distortion_for_training\n",
    "        dataset = bcl.BCLDataSet(data_dir, subset, use_distortion)\n",
    "        image_batch, label_batch = dataset.make_batch(batch_size)\n",
    "        if num_shards <= 1:\n",
    "            # No GPU available or only 1 GPU.\n",
    "            return [image_batch], [label_batch]\n",
    "\n",
    "        # Note that passing num=batch_size is safe here, even though\n",
    "        # dataset.batch(batch_size) can, in some cases, return fewer than batch_size\n",
    "        # examples. This is because it does so only when repeating for a limited\n",
    "        # number of epochs, but our dataset repeats forever.\n",
    "        image_batch = tf.unstack(image_batch, num=batch_size, axis=0)\n",
    "        label_batch = tf.unstack(label_batch, num=batch_size, axis=0)\n",
    "        image_shards = [[] for i in range(num_shards)]\n",
    "        label_shards = [[] for i in range(num_shards)]\n",
    "        for i in range(batch_size):\n",
    "            idx = i % num_shards\n",
    "            image_shards[idx].append(image_batch[i])\n",
    "            label_shards[idx].append(label_batch[i])\n",
    "        image_shards = [tf.parallel_stack(x) for x in image_shards]\n",
    "        label_shards = [tf.parallel_stack(x) for x in label_shards]\n",
    "        return image_shards, label_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_fn(data_dir,\n",
    "                      num_gpus,\n",
    "                      variable_strategy,\n",
    "                      use_distortion_for_training=True):\n",
    "    \"\"\"Returns an Experiment function.\n",
    "    Experiments perform training on several workers in parallel,\n",
    "    in other words experiments know how to invoke train and eval in a sensible\n",
    "    fashion for distributed training. Arguments passed directly to this\n",
    "    function are not tunable, all other arguments should be passed within\n",
    "    tf.HParams, passed to the enclosed function.\n",
    "    Args:\n",
    "      data_dir: str. Location of the data for input_fns.\n",
    "      num_gpus: int. Number of GPUs on each worker.\n",
    "      variable_strategy: String. CPU to use CPU as the parameter server\n",
    "      and GPU to use the GPUs as the parameter server.\n",
    "      use_distortion_for_training: bool. See cifar10.Cifar10DataSet.\n",
    "    Returns:\n",
    "      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n",
    "      tf.contrib.learn.Experiment.\n",
    "      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n",
    "      methods on Experiment (train, evaluate) based on information\n",
    "      about the current runner in `run_config`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _experiment_fn(run_config, hparams):\n",
    "        \"\"\"Returns an Experiment.\"\"\"\n",
    "        # Create estimator.\n",
    "        train_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='train',\n",
    "            num_shards=num_gpus,\n",
    "            batch_size=hparams.train_batch_size,\n",
    "            use_distortion_for_training=use_distortion_for_training)\n",
    "\n",
    "        val_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='val',\n",
    "            batch_size=hparams.val_batch_size,\n",
    "            num_shards=num_gpus)\n",
    "\n",
    "        train_steps = hparams.train_steps\n",
    "\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=get_model_fn(num_gpus, variable_strategy,\n",
    "                                  run_config.num_worker_replicas or 1),\n",
    "                                  config=run_config,\n",
    "                                  params=hparams)\n",
    "\n",
    "        #train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=train_steps)\n",
    "        #eval_spec = tf.estimator.EvalSpec(input_fn=val_input_fn)\n",
    "        #tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "        # Create experiment.\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            estimator,\n",
    "            train_input_fn=train_input_fn,\n",
    "            eval_input_fn=val_input_fn,\n",
    "            train_steps=train_steps,\n",
    "            eval_steps=10,\n",
    "            min_eval_frequency=10)\n",
    "\n",
    "    return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(job_dir, data_dir, num_gpus, variable_strategy,\n",
    "         use_distortion_for_training, log_device_placement, num_intra_threads,\n",
    "         **hparams):\n",
    "    # The env variable is on deprecation path, default is set to off.\n",
    "    os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "    # Session configuration.\n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=log_device_placement,\n",
    "        intra_op_parallelism_threads=num_intra_threads,\n",
    "        gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n",
    "\n",
    "    config = cifar10_utils.RunConfig(\n",
    "        session_config=sess_config, model_dir=job_dir, save_checkpoints_steps=10)\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        get_experiment_fn(data_dir, num_gpus, variable_strategy,\n",
    "                          use_distortion_for_training),\n",
    "        run_config=config,\n",
    "        schedule=\"train_and_evaluate\",\n",
    "        hparams=tf.contrib.training.HParams(\n",
    "            is_chief=config.is_chief,\n",
    "            **hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"sequence_data/12\",\n",
    "    \"job_dir\": \"tmp/bcl\",\n",
    "    \"variable_strategy\": \"CPU\",\n",
    "    \"num_gpus\": len(used_gpus),\n",
    "    \"num_layers\": 4,\n",
    "    \"train_steps\": 14000,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 128,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 2e-4,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"use_distortion_for_training\": False,\n",
    "    \"sync\": False,\n",
    "    \"num_intra_threads\": 0,\n",
    "    \"num_inter_threads\": 0,\n",
    "    \"data_format\": \"channels_last\",\n",
    "    \"log_device_placement\": False,\n",
    "    \"batch_norm_decay\": 0.997,\n",
    "    \"batch_norm_epsilon\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbcfef66cc0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'tmp/bcl'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-13000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 13000 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:18:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-13000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:18:31\n",
      "INFO:tensorflow:Saving dict for global step 13000: global_step = 13000, loss = 0.27871346, mse = 0.27871335\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 13000: tmp/bcl/model.ckpt-13000\n",
      "INFO:tensorflow:Validation (step 13001): loss = 0.27871346, mse = 0.27871335, global_step = 13000\n",
      "INFO:tensorflow:loss = 0.17564362, step = 13000\n",
      "INFO:tensorflow:loss = 0.17564362\n",
      "INFO:tensorflow:Average examples/sec: 118.86 (118.86), step = 10\n",
      "INFO:tensorflow:Average examples/sec: 217.38 (1270.36), step = 20\n",
      "INFO:tensorflow:Average examples/sec: 300.964 (1302.92), step = 30\n",
      "INFO:tensorflow:Average examples/sec: 372.56 (1301.12), step = 40\n",
      "INFO:tensorflow:Average examples/sec: 434.855 (1313.11), step = 50\n",
      "INFO:tensorflow:Average examples/sec: 488.115 (1259.28), step = 60\n",
      "INFO:tensorflow:Average examples/sec: 535.092 (1266.34), step = 70\n",
      "INFO:tensorflow:Average examples/sec: 577.488 (1296.63), step = 80\n",
      "INFO:tensorflow:Average examples/sec: 616.107 (1324.93), step = 90\n",
      "INFO:tensorflow:global_step/sec: 2.70542\n",
      "INFO:tensorflow:loss = 0.18277955, step = 13100 (19.689 sec)\n",
      "INFO:tensorflow:loss = 0.18277955 (19.689 sec)\n",
      "INFO:tensorflow:Average examples/sec: 650.108 (1291.62), step = 100\n",
      "INFO:tensorflow:Average examples/sec: 680.307 (1270.49), step = 110\n",
      "INFO:tensorflow:Average examples/sec: 709.067 (1325.4), step = 120\n",
      "INFO:tensorflow:Average examples/sec: 734.112 (1274.17), step = 130\n",
      "INFO:tensorflow:Average examples/sec: 756.991 (1272.6), step = 140\n",
      "INFO:tensorflow:Average examples/sec: 778.425 (1289.62), step = 150\n",
      "INFO:tensorflow:Average examples/sec: 798.477 (1301.3), step = 160\n",
      "INFO:tensorflow:Average examples/sec: 817.446 (1318.7), step = 170\n",
      "INFO:tensorflow:Average examples/sec: 834.969 (1313.71), step = 180\n",
      "INFO:tensorflow:Average examples/sec: 851.566 (1326.01), step = 190\n",
      "INFO:tensorflow:global_step/sec: 10.1734\n",
      "INFO:tensorflow:loss = 0.17764899, step = 13200 (9.832 sec)\n",
      "INFO:tensorflow:loss = 0.17764899 (9.832 sec)\n",
      "INFO:tensorflow:Average examples/sec: 867.167 (1330.16), step = 200\n",
      "INFO:tensorflow:Average examples/sec: 881.067 (1296.83), step = 210\n",
      "INFO:tensorflow:Average examples/sec: 893.357 (1263.45), step = 220\n",
      "INFO:tensorflow:Average examples/sec: 905.077 (1272.27), step = 230\n",
      "INFO:tensorflow:Average examples/sec: 916.345 (1284.02), step = 240\n",
      "INFO:tensorflow:Average examples/sec: 927.463 (1308.49), step = 250\n",
      "INFO:tensorflow:Average examples/sec: 938.088 (1314.58), step = 260\n",
      "INFO:tensorflow:Average examples/sec: 948.168 (1315.79), step = 270\n",
      "INFO:tensorflow:Average examples/sec: 958.097 (1335.75), step = 280\n",
      "INFO:tensorflow:Average examples/sec: 967.571 (1338.05), step = 290\n",
      "INFO:tensorflow:global_step/sec: 10.1993\n",
      "INFO:tensorflow:loss = 0.1656192, step = 13300 (9.802 sec)\n",
      "INFO:tensorflow:loss = 0.1656192 (9.801 sec)\n",
      "INFO:tensorflow:Average examples/sec: 976.539 (1335.48), step = 300\n",
      "INFO:tensorflow:Average examples/sec: 984.473 (1301.77), step = 310\n",
      "INFO:tensorflow:Average examples/sec: 990.933 (1244), step = 320\n",
      "INFO:tensorflow:Average examples/sec: 997.511 (1266.56), step = 330\n",
      "INFO:tensorflow:Average examples/sec: 1004.25 (1292.35), step = 340\n",
      "INFO:tensorflow:Average examples/sec: 1010.73 (1294.68), step = 350\n",
      "INFO:tensorflow:Average examples/sec: 1017.09 (1304.28), step = 360\n",
      "INFO:tensorflow:Average examples/sec: 1022.56 (1268.42), step = 370\n",
      "INFO:tensorflow:Average examples/sec: 1027.62 (1257.61), step = 380\n",
      "INFO:tensorflow:Average examples/sec: 1032.78 (1276.3), step = 390\n",
      "INFO:tensorflow:global_step/sec: 10.0016\n",
      "INFO:tensorflow:loss = 0.1686337, step = 13400 (9.999 sec)\n",
      "INFO:tensorflow:loss = 0.1686337 (9.999 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1038.09 (1298.57), step = 400\n",
      "INFO:tensorflow:Average examples/sec: 1043.05 (1289.35), step = 410\n",
      "INFO:tensorflow:Average examples/sec: 1048.13 (1309.99), step = 420\n",
      "INFO:tensorflow:Average examples/sec: 1052.11 (1251.61), step = 430\n",
      "INFO:tensorflow:Average examples/sec: 1056.17 (1266.43), step = 440\n",
      "INFO:tensorflow:Average examples/sec: 1060.31 (1281.32), step = 450\n",
      "INFO:tensorflow:Average examples/sec: 1064.34 (1284.14), step = 460\n",
      "INFO:tensorflow:Average examples/sec: 1068.61 (1310.28), step = 470\n",
      "INFO:tensorflow:Average examples/sec: 1072.8 (1315.29), step = 480\n",
      "INFO:tensorflow:Average examples/sec: 1076.87 (1316.37), step = 490\n",
      "INFO:tensorflow:global_step/sec: 10.0629\n",
      "INFO:tensorflow:loss = 0.17059971, step = 13500 (9.938 sec)\n",
      "INFO:tensorflow:loss = 0.17059971 (9.938 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1080 (1259.44), step = 500\n",
      "INFO:tensorflow:Average examples/sec: 1083.13 (1266.75), step = 510\n",
      "INFO:tensorflow:Average examples/sec: 1086.57 (1296.48), step = 520\n",
      "INFO:tensorflow:Average examples/sec: 1089.91 (1296.86), step = 530\n",
      "INFO:tensorflow:Average examples/sec: 1093.44 (1320.43), step = 540\n",
      "INFO:tensorflow:Average examples/sec: 1096.69 (1306.05), step = 550\n",
      "INFO:tensorflow:Average examples/sec: 1099.97 (1316.77), step = 560\n",
      "INFO:tensorflow:Average examples/sec: 1102.93 (1298.32), step = 570\n",
      "INFO:tensorflow:Average examples/sec: 1105.22 (1253.71), step = 580\n",
      "INFO:tensorflow:Average examples/sec: 1107.62 (1267.25), step = 590\n",
      "INFO:tensorflow:global_step/sec: 10.085\n",
      "INFO:tensorflow:loss = 0.18276466, step = 13600 (9.916 sec)\n",
      "INFO:tensorflow:loss = 0.18276466 (9.915 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1110.24 (1290.49), step = 600\n",
      "INFO:tensorflow:Average examples/sec: 1112.87 (1297.01), step = 610\n",
      "INFO:tensorflow:Average examples/sec: 1115.45 (1299.02), step = 620\n",
      "INFO:tensorflow:Average examples/sec: 1118.23 (1323.25), step = 630\n",
      "INFO:tensorflow:Average examples/sec: 1120.54 (1288.36), step = 640\n",
      "INFO:tensorflow:Average examples/sec: 1122.39 (1254.32), step = 650\n",
      "INFO:tensorflow:Average examples/sec: 1124.51 (1282.37), step = 660\n",
      "INFO:tensorflow:Average examples/sec: 1126.66 (1289.49), step = 670\n",
      "INFO:tensorflow:Average examples/sec: 1129.07 (1317.6), step = 680\n",
      "INFO:tensorflow:Average examples/sec: 1131.32 (1309.08), step = 690\n",
      "INFO:tensorflow:global_step/sec: 10.1441\n",
      "INFO:tensorflow:loss = 0.17852929, step = 13700 (9.858 sec)\n",
      "INFO:tensorflow:loss = 0.17852929 (9.858 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1133.71 (1326.43), step = 700\n",
      "INFO:tensorflow:Average examples/sec: 1135.92 (1315.73), step = 710\n",
      "INFO:tensorflow:Average examples/sec: 1137.33 (1246.94), step = 720\n",
      "INFO:tensorflow:Average examples/sec: 1139.02 (1275.9), step = 730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average examples/sec: 1140.92 (1299.3), step = 740\n",
      "INFO:tensorflow:Average examples/sec: 1142.75 (1296.42), step = 750\n",
      "INFO:tensorflow:Average examples/sec: 1144.73 (1315.7), step = 760\n",
      "INFO:tensorflow:Average examples/sec: 1146.65 (1314.04), step = 770\n",
      "INFO:tensorflow:Average examples/sec: 1148.43 (1304.2), step = 780\n",
      "INFO:tensorflow:Average examples/sec: 1150.42 (1330.96), step = 790\n",
      "INFO:tensorflow:global_step/sec: 10.141\n",
      "INFO:tensorflow:loss = 0.16023386, step = 13800 (9.861 sec)\n",
      "INFO:tensorflow:loss = 0.16023386 (9.861 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1151.94 (1285.77), step = 800\n",
      "INFO:tensorflow:Average examples/sec: 1153.17 (1260.99), step = 810\n",
      "INFO:tensorflow:Average examples/sec: 1154.5 (1272.97), step = 820\n",
      "INFO:tensorflow:Average examples/sec: 1156.06 (1301.01), step = 830\n",
      "INFO:tensorflow:Average examples/sec: 1157.68 (1309.46), step = 840\n",
      "INFO:tensorflow:Average examples/sec: 1159.33 (1317.5), step = 850\n",
      "INFO:tensorflow:Average examples/sec: 1160.94 (1315.82), step = 860\n",
      "INFO:tensorflow:Average examples/sec: 1162.12 (1273.26), step = 870\n",
      "INFO:tensorflow:Average examples/sec: 1163.16 (1261.63), step = 880\n",
      "INFO:tensorflow:Average examples/sec: 1164.42 (1286.8), step = 890\n",
      "INFO:tensorflow:global_step/sec: 10.0783\n",
      "INFO:tensorflow:loss = 0.15291567, step = 13900 (9.923 sec)\n",
      "INFO:tensorflow:loss = 0.15291567 (9.923 sec)\n",
      "INFO:tensorflow:Average examples/sec: 1165.79 (1302.91), step = 900\n",
      "INFO:tensorflow:Average examples/sec: 1167.32 (1322.65), step = 910\n",
      "INFO:tensorflow:Average examples/sec: 1168.79 (1320.78), step = 920\n",
      "INFO:tensorflow:Average examples/sec: 1170.25 (1322.5), step = 930\n",
      "INFO:tensorflow:Average examples/sec: 1171.21 (1267.01), step = 940\n",
      "INFO:tensorflow:Average examples/sec: 1172.18 (1271.5), step = 950\n",
      "INFO:tensorflow:Average examples/sec: 1173.24 (1283.48), step = 960\n",
      "INFO:tensorflow:Average examples/sec: 1174.35 (1291.97), step = 970\n",
      "INFO:tensorflow:Average examples/sec: 1175.62 (1313.53), step = 980\n",
      "INFO:tensorflow:Average examples/sec: 1176.66 (1287.7), step = 990\n",
      "INFO:tensorflow:Saving checkpoints for 14000 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.16564417.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:20:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:20:39\n",
      "INFO:tensorflow:Saving dict for global step 14000: global_step = 14000, loss = 0.27150685, mse = 0.2715068\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14000: tmp/bcl/model.ckpt-14000\n"
     ]
    }
   ],
   "source": [
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
