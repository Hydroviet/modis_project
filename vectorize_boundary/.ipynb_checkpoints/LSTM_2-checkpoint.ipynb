{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Process\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, LSTM, BatchNormalization\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modis_utils.misc import cache_data, restore_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = restore_data(os.path.join('cache', 'boundary_vectors_ALL_1.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 1657, 2)\n",
      "(92, 1657, 2)\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_boundary_vectors = data[0]\n",
    "test_boundary_vectors = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((598, 1657, 2), (92, 1657, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_boundary_vectors.shape, test_boundary_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = train_boundary_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_years = len(train_boundary_vectors)//46\n",
    "n_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_train = train_boundary_vectors[:0].copy()\\ndata_test = test_boundary_vectors\\nfor i in range(n_years):\\n    year = 2003 + i\\n    if year != 2011 and year != 2013:\\n        data_train = np.vstack([data_train, train_boundary_vectors[i*46 : (i + 1)*46]])\\nprint(data_train.shape)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_train = train_boundary_vectors[:0].copy()\n",
    "data_test = test_boundary_vectors\n",
    "for i in range(n_years):\n",
    "    year = 2003 + i\n",
    "    if year != 2011 and year != 2013:\n",
    "        data_train = np.vstack([data_train, train_boundary_vectors[i*46 : (i + 1)*46]])\n",
    "print(data_train.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_boundary_vectors\n",
    "data_test = test_boundary_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 3314)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_1 = data_train.reshape(data_train.shape[0], -1)\n",
    "data_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_1 = data_test.reshape(data_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = []\n",
    "for i in range(data_train_1.shape[1]):\n",
    "    var = np.var(data_train_1[:, i])\n",
    "    variants.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_1 = variants.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , 16.40228018,  0.        , ..., 16.31268386,\n",
       "        0.        , 17.84837977])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variants_2 = np.asarray(variants_1)\n",
    "variants_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_idx = np.where(variants_2 > 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2311"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 2311)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_2 = data_train_1[:, list_idx]\n",
    "data_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 2311)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_2 = data_test_1[:, list_idx]\n",
    "data_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhkhoa/anaconda3/envs/pytf/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_3 = scaler.transform(data_train_2)\n",
    "data_test_3 = scaler.transform(data_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_data(data_train, data_test, timesteps=47):\n",
    "    data_all = np.vstack([data_train, data_test])\n",
    "    len_val = 46\n",
    "    len_train = len(data_train) - timesteps - len_val\n",
    "    len_test = len(data_test)\n",
    "    \n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    val_X = []\n",
    "    val_y = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    \n",
    "    for i in range(timesteps, timesteps + len_train):\n",
    "        X = np.expand_dims(data_all[i - timesteps : i], axis=0)\n",
    "        y = np.expand_dims(data_all[i], axis=0)\n",
    "        train_X.append(X)\n",
    "        train_y.append(y)\n",
    "        \n",
    "    for i in range(timesteps + len_train, timesteps + len_train + len_val):\n",
    "        X = np.expand_dims(data_all[i - timesteps : i], axis=0)\n",
    "        y = np.expand_dims(data_all[i], axis=0)\n",
    "        val_X.append(X)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for i in range(timesteps + len_train + len_val, timesteps + len_train + len_val + len_test):\n",
    "        X = np.expand_dims(data_all[i - timesteps : i], axis=0)\n",
    "        y = np.expand_dims(data_all[i], axis=0)\n",
    "        test_X.append(X)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    return np.vstack(train_X), np.vstack(train_y), np.vstack(val_X), np.vstack(val_y), np.vstack(test_X), np.vstack(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, val_X, val_y, test_X, test_y = create_sequence_data(data_train_3, data_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['train_X'] = train_X\n",
    "data['train_y'] = train_y\n",
    "data['val_X'] = val_X\n",
    "data['val_y'] = val_y\n",
    "data['test_X'] = test_X\n",
    "data['test_y'] = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0000000000000142\n",
      "0.0 1.0000000000000142\n",
      "0.0 1.0000000000000142\n",
      "0.0 1.0000000000000142\n",
      "0.0 1.0000000000000142\n",
      "0.0 1.0000000000000142\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    print(v.min(), v.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505, 47, 2311)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_pickle(data, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_data_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def create_empty_list(n):\n",
    "    res = []\n",
    "    for _ in range(n):\n",
    "        res.append(None)\n",
    "    return res\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.mean((x - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fig_dir = 'visualize/lstm_2/training'\n",
    "if not os.path.exists(training_fig_dir):\n",
    "    os.makedirs(training_fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tensorflow.python.client import device_lib\\n\\ndef get_available_gpus():\\n    local_device_protos = device_lib.list_local_devices()\\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\\nget_available_gpus()\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dir = 'inference'\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedir(inference_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gpu_id, data_path, epochs, batch_size, timesteps, units, n, model_dir, training_fig_dir):\n",
    "    subprocess.call(['python', 'lstm_train.py', \n",
    "                     str(gpu_id), data_path, str(epochs), str(batch_size), \n",
    "                     str(timesteps), str(units), str(n), model_dir, training_fig_dir])\n",
    "\n",
    "def inference(gpu_id, input_path, batch_size, steps, model_paths_path, n):\n",
    "    subprocess.call(['python', 'lstm_inference.py', \n",
    "                     str(gpu_id), input_path, str(batch_size), str(steps), model_paths_path, str(n)])\n",
    "\n",
    "class LSTM_2:\n",
    "\n",
    "    def __init__(self, data, data_train_full, list_idx=None, scaler=None, mode='train'):\n",
    "        self.model_dir = 'lstm'\n",
    "        self.mode = mode\n",
    "        self.model = None\n",
    "        self.n_cores = 4\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.units = data['train_X'].shape[2]\n",
    "        self.timesteps = data['train_X'].shape[1]\n",
    "        self.full_shape = data_train_full.shape[-1]\n",
    "        \n",
    "        if list_idx is None:\n",
    "            self.list_idx = np.arange(self.full_shape)\n",
    "        self.list_idx = list_idx\n",
    "\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "            \n",
    "        if self.mode == 'inference':\n",
    "            self.model_paths = self.load_model(self.model_dir)\n",
    "            \n",
    "        self.list_small_variant_idx = np.setdiff1d(np.arange(self.full_shape), self.list_idx)\n",
    "        self.default_values = {}\n",
    "        for i in self.list_small_variant_idx:\n",
    "            mean_value = np.mean(data_train_full[:, i])\n",
    "            self.default_values[i] = int(mean_value)\n",
    "            \n",
    "        self.scaler = scaler\n",
    "        self.model_paths = None\n",
    "        \n",
    "    def load_model(self, model_dir=None):\n",
    "        if model_dir is None:\n",
    "            model_dir=self.model_dir\n",
    "        self.model_paths = {}\n",
    "        for i in range(len(self.list_idx)):\n",
    "            self.model_paths[i] = os.path.join(model_dir, '{}.dat'.format(i))\n",
    "        return self.model_paths\n",
    "    \n",
    "    def train(self, epochs=1, batch_size=32):\n",
    "        if not os.path.exists('tmp'):\n",
    "            os.makedirs('tmp')\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        cache_data(self.data, 'tmp/data.dat')\n",
    "        processes = []\n",
    "        for gpu_id in range(4):\n",
    "            p = Process(target=train, args=(gpu_id, 'tmp/data.dat', epochs, batch_size,\n",
    "                                            self.timesteps, 1, len(self.list_idx),\n",
    "                                            self.model_dir, training_fig_dir))\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        self.model_paths = self.load_model()\n",
    "        return self.model_paths\n",
    "    \n",
    "    def inference(self, input_test, return_full=None, return_original_range=False):\n",
    "        res = self.inference_multisteps(input_test, 1, return_full, return_original_range)\n",
    "    \n",
    "    def _inverse_transform(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.reshape(-1, shape[-1])\n",
    "        x = self.scaler.inverse_transform(x)\n",
    "        return x.reshape(shape)\n",
    "    \n",
    "    def _predict_multisteps(self, input_test, steps, batch_size=128):\n",
    "        if not os.path.exists('tmp'):\n",
    "            os.makedirs('tmp')\n",
    "        cache_data(input_test, 'tmp/input_test.dat')\n",
    "        cache_data(self.model_paths, 'tmp/model_paths.dat')\n",
    "        processes = []\n",
    "        for gpu_id in range(4):\n",
    "            p = Process(target=inference, args=(gpu_id, 'tmp/input_test.dat', batch_size,\n",
    "                                                steps, 'tmp/model_paths.dat', len(self.list_idx)))\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        res = []\n",
    "        for i in range(4):\n",
    "            res += restore_data('tmp/out_{}.dat'.format(i))\n",
    "        return np.concatenate(res, axis=-1)\n",
    "    \n",
    "    def inference_multisteps(self, input_test, steps=1, return_full=None, return_original_range=False):\n",
    "        if self.model is None:\n",
    "            self.model = self.load_model()\n",
    "        if len(input_test.shape) == 2:\n",
    "            input_test = np.expand_dims(input_test, axis=0)\n",
    "        inferences = self._predict_multisteps(input_test, batch_size=64, steps=steps)\n",
    "        if return_full:\n",
    "            inferences = self._inverse_transform(inferences)\n",
    "            #inferences = (92x80x2285)\n",
    "            outputs = np.zeros((inferences.shape[:-1] + (self.full_shape,)))\n",
    "            for i, idx in enumerate(self.list_idx):\n",
    "                outputs[:, :, idx] = inferences[:, :, i]\n",
    "            for idx, default_value in self.default_values.items():\n",
    "                outputs[:, :, idx] = default_value\n",
    "            return outputs\n",
    "        if return_original_range:\n",
    "            inferences = self._inverse_transform(inferences)\n",
    "        return inferences\n",
    "\n",
    "    def eval(self, inputs=None, groundtruths=None, return_original_range=False):\n",
    "        if inputs is None:\n",
    "            inputs = self.data['test_X']\n",
    "        if groundtruths is None:\n",
    "            groundtruths = self.data['test_y']\n",
    "        groundtruth_shape = groundtruths.shape[-1]\n",
    "        return_full = (groundtruth_shape == self.full_shape)\n",
    "        predictions = self.inference(inputs, return_full, return_original_range)\n",
    "        if return_original_range:\n",
    "            groundtruths = self.scaler.inverse_transform(groundtruths)\n",
    "        \n",
    "        loss = (groundtruths - predictions)**2\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2 = LSTM_2(data, data_train_1, list_idx, scaler, mode='train')\n",
    "model_path = lstm_2.train(epochs=30, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2 = LSTM_2(data, data_train_1, list_idx, scaler, mode='inference')\n",
    "full_shape_predictions_multi_steps = lstm_2.inference_multisteps(data['test_X'], 80, return_full=True)\n",
    "cache_data(full_shape_predictions_multi_steps, os.path.join(inference_dir, 'lstm_2_80_multisteps.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 80, 3314)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_shape_predictions_multi_steps = restore_data(os.path.join(inference_dir, 'lstm_2_80_multisteps.dat'))\n",
    "full_shape_predictions_multi_steps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-be2b50570845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_shape_predictions_multi_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "predictions = full_shape_predictions_multi_steps[:, 0, :]\n",
    "losses = mse(data_test['test_y'], predictions)\n",
    "print(losses)\n",
    "print(losses.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_point(x):\n",
    "    return x.reshape(x.shape[0], -1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_shape_predictions_multi_steps_0 = reshape_to_point(full_shape_predictions_multi_steps[0, :, :])\n",
    "full_shape_predictions_multi_steps_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boundaries_to_image(boundary, img_width, img_height):\n",
    "    img = np.zeros((img_width, img_height))\n",
    "    for i in range(boundary.shape[0]):\n",
    "        x = boundary[i][0].astype(np.int32)\n",
    "        y = boundary[i][1].astype(np.int32)\n",
    "        img[x, y] = 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = convert_boundaries_to_image(full_shape_predictions_multi_steps_0[0], 513, 513)\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Polygon area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_shape_predictions_multi_steps.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_border(data_points):\n",
    "    x = data_points[:, 0]\n",
    "    y = data_points[:, 1]\n",
    "    return x.min(), y.min(), x.max(), y.max()\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, x1, y1, x2, y2):\n",
    "        self.a = y1 - y2\n",
    "        self.b = x2 - x1\n",
    "        self.c = x1*y2 - x2*y1\n",
    "    def calc(self, x, y):\n",
    "        return self.a*x + self.b*y + self.c\n",
    "    \n",
    "def convert_boundary_vector_to_polygon(boundary_vector):\n",
    "    x1, y1, x2, y2 = find_border(boundary_vector)\n",
    "    line = Line(x1, y1, x2, y2)\n",
    "    score_point = [line.calc(x, y) for x, y in boundary_vector]\n",
    "    group_1 = []\n",
    "    group_2 = []\n",
    "    for i, p in enumerate(boundary_vector):\n",
    "        if score_point[i] < 0:\n",
    "            group_1.append(p)\n",
    "        else:\n",
    "            group_2.append(p)\n",
    "    group_1 = sorted(group_1, key=itemgetter(0,1))\n",
    "    group_2 = sorted(group_2, key=itemgetter(0,1), reverse=True)\n",
    "    group = np.vstack([group_1, group_2])\n",
    "    return Polygon(zip(group[:, 0], group[:, 1]))\n",
    "\n",
    "region = [(10, 10), (200, 10), (512, 400), (512, 512), (300, 512), (10, 100), (10, 10)]\n",
    "\n",
    "def check_point_in_region(p, lines, sample_scores):\n",
    "    for line, sample_score in zip(lines, sample_scores):\n",
    "        score = line.calc(p[0], p[1])\n",
    "        if score/sample_score < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_points_in_region(boundary, region):\n",
    "    lines = []\n",
    "    sample_point = (256, 256)\n",
    "    for i in range(len(region) - 1):\n",
    "        lines.append(Line(*(region[i] + region[i + 1])))\n",
    "    res = []\n",
    "    sample_scores = [line.calc(sample_point[0], sample_point[1]) for line in lines]\n",
    "    for p in boundary:\n",
    "        if check_point_in_region(p, lines, sample_scores):\n",
    "            res.append(p)\n",
    "    return np.vstack(res)\n",
    "\n",
    "def calc_area(points, region):\n",
    "    points = get_points_in_region(points, region)\n",
    "    p = convert_boundary_vector_to_polygon(points)\n",
    "    pp = p.buffer(0)\n",
    "    return pp.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_area(full_shape_predictions_multi_steps_0[0], region))\n",
    "print(calc_area(data_test[0], region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_area(full_shape_predictions_multi_steps_0[1], region))\n",
    "print(calc_area(data_test[1], region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_lstm = []\n",
    "for boundary_vector in full_shape_predictions_multi_steps_0:\n",
    "    area = calc_area(boundary_vector, region)\n",
    "    area_lstm.append(area*0.25*0.25)\n",
    "    \n",
    "area_test_truth = []\n",
    "for boundary_vector in data_test:\n",
    "    area = calc_area(boundary_vector, region)\n",
    "    area_test_truth.append(area*0.25*0.25)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15, 10))\n",
    "ax.plot(area_lstm, color='green', label='area_lstm')\n",
    "ax.plot(area_test_truth, color='r', label='area_test_truth')\n",
    "ax.set_xlabel('timesteps')\n",
    "ax.set_ylabel('area (km^2)')\n",
    "ax.legend()\n",
    "plt.savefig('visualize/area_groundtruth_lstm_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
