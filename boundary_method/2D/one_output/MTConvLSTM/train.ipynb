{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcl\n",
    "import bcl_model\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"0,1,2,3\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "used_gpus = [0, 1, 2, 3]\n",
    "s = str(used_gpus[0])\n",
    "for used_gpu in used_gpus[1:]:\n",
    "    s += ',{}'.format(used_gpu)\n",
    "\n",
    "s1 = \"\\\"{}\\\"\".format(s)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10_utils\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fn(num_gpus, variable_strategy, num_workers):\n",
    "    \"\"\"Returns a function that will build the resnet model.\"\"\"\n",
    "\n",
    "    def _bcl_model_fn(features, labels, mode, params):\n",
    "        \"\"\"Resnet model body.\n",
    "        Support single host, one or more GPU training. Parameter distribution can\n",
    "        be either one of the following scheme.\n",
    "        1. CPU is the parameter server and manages gradient updates.\n",
    "        2. Parameters are distributed evenly across all GPUs, and the first GPU\n",
    "           manages gradient updates.\n",
    "        Args:\n",
    "          features: a list of tensors, one for each tower\n",
    "          labels: a list of tensors, one for each tower\n",
    "          mode: ModeKeys.TRAIN or EVAL\n",
    "          params: Hyperparameters suitable for tuning\n",
    "        Returns:\n",
    "          A EstimatorSpec object.\n",
    "        \"\"\"\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        weight_decay = params.weight_decay\n",
    "        momentum = params.momentum\n",
    "\n",
    "        tower_inputs = features\n",
    "        tower_groundtruths = labels\n",
    "        tower_losses = []\n",
    "        tower_gradvars = []\n",
    "        tower_preds = []\n",
    "\n",
    "        # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n",
    "        # on CPU. The exception is Intel MKL on CPU which is optimal with\n",
    "        # channels_last.\n",
    "        data_format = params.data_format\n",
    "        if not data_format:\n",
    "            if num_gpus == 0:\n",
    "                data_format = 'channels_last'\n",
    "            else:\n",
    "                data_format = 'channels_first'\n",
    "\n",
    "        if num_gpus == 0:\n",
    "            num_devices = 1\n",
    "            device_type = 'cpu'\n",
    "        else:\n",
    "            num_devices = num_gpus\n",
    "            device_type = 'gpu'\n",
    "\n",
    "        for i in range(num_devices):\n",
    "            worker_device = '/{}:{}'.format(device_type, i)\n",
    "            if variable_strategy == 'CPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    worker_device=worker_device)\n",
    "            elif variable_strategy == 'GPU':\n",
    "                device_setter = cifar10_utils.local_device_setter(\n",
    "                    ps_device_type='gpu',\n",
    "                    worker_device=worker_device,\n",
    "                    ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n",
    "                        num_gpus, tf.contrib.training.byte_size_load_fn))\n",
    "            with tf.variable_scope('bcl', reuse=bool(i != 0)):\n",
    "                with tf.name_scope('tower_%d' % i) as name_scope:\n",
    "                    with tf.device(device_setter):\n",
    "                        loss, gradvars, preds = _tower_fn(\n",
    "                            is_training, weight_decay, tower_inputs[i], tower_groundtruths[i],\n",
    "                            data_format, params.num_layers, params.batch_norm_decay,\n",
    "                            params.batch_norm_epsilon)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_gradvars.append(gradvars)\n",
    "                        tower_preds.append(preds)\n",
    "                        if i == 0:\n",
    "                            # Only trigger batch_norm moving mean and variance update from\n",
    "                            # the 1st tower. Ideally, we should grab the updates from all\n",
    "                            # towers but these stats accumulate extremely fast so we can\n",
    "                            # ignore the other stats from the other towers without\n",
    "                            # significant detriment.\n",
    "                            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n",
    "                                                           name_scope)\n",
    "\n",
    "        # Now compute global loss and gradients.\n",
    "        gradvars = []\n",
    "        with tf.name_scope('gradient_averaging'):\n",
    "            all_grads = {}\n",
    "            for grad, var in itertools.chain(*tower_gradvars):\n",
    "                if grad is not None:\n",
    "                    all_grads.setdefault(var, []).append(grad)\n",
    "            for var, grads in six.iteritems(all_grads):\n",
    "                # Average gradients on the same device as the variables\n",
    "                # to which they apply.\n",
    "                with tf.device(var.device):\n",
    "                    if len(grads) == 1:\n",
    "                        avg_grad = grads[0]\n",
    "                    else:\n",
    "                        avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n",
    "                gradvars.append((avg_grad, var))\n",
    "\n",
    "        # Device that runs the ops to apply global gradient updates.\n",
    "        consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'\n",
    "        with tf.device(consolidation_device):\n",
    "            # Suggested learning rate scheduling from\n",
    "            # https://github.com/ppwwyyxx/tensorpack/blob/master/examples/ResNet/cifar10-resnet.py#L155\n",
    "            num_batches_per_epoch = bcl.BCLDataSet.num_examples_per_epoch(\n",
    "                'train') // (params.train_batch_size * num_workers)\n",
    "            boundaries = [\n",
    "                num_batches_per_epoch * x\n",
    "                for x in np.array([82, 123, 300], dtype=np.int64)\n",
    "            ]\n",
    "            staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]\n",
    "\n",
    "            learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n",
    "                                                        boundaries, staged_lr)\n",
    "\n",
    "            loss = tf.reduce_mean(tower_losses, name='loss')\n",
    "\n",
    "            examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(\n",
    "                params.train_batch_size, every_n_steps=10)\n",
    "            \n",
    "            optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate=params.learning_rate, momentum=momentum)\n",
    "\n",
    "            tensors_to_log = {'loss': loss}\n",
    "\n",
    "            logging_hook = tf.train.LoggingTensorHook(\n",
    "                tensors=tensors_to_log, every_n_iter=100)\n",
    "\n",
    "            train_hooks = [logging_hook, examples_sec_hook]\n",
    "\n",
    "            if params.sync:\n",
    "                optimizer = tf.train.SyncReplicasOptimizer(\n",
    "                    optimizer, replicas_to_aggregate=num_workers)\n",
    "                sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n",
    "                train_hooks.append(sync_replicas_hook)\n",
    "\n",
    "            # Create single grouped train op\n",
    "            train_op = [\n",
    "                optimizer.apply_gradients(\n",
    "                    gradvars, global_step=tf.train.get_global_step())\n",
    "            ]\n",
    "            train_op.extend(update_ops)\n",
    "            train_op = tf.group(*train_op)\n",
    "\n",
    "            predictions = tf.concat(tower_preds, axis=0)\n",
    "            groundtruths = tf.concat(labels, axis=0)\n",
    "            metrics = {\n",
    "                'mse':\n",
    "                    tf.metrics.mean_squared_error(groundtruths, predictions)\n",
    "            }\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            training_hooks=train_hooks,\n",
    "            eval_metric_ops=metrics)\n",
    "\n",
    "    return _bcl_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tower_fn(is_training, weight_decay, inputs, groundtruths, data_format,\n",
    "              num_layers, batch_norm_decay, batch_norm_epsilon):\n",
    "    \"\"\"Build computation tower (Resnet).\n",
    "    Args:\n",
    "    is_training: true if is training graph.\n",
    "    weight_decay: weight regularization strength, a float.\n",
    "    feature: a Tensor.\n",
    "    label: a Tensor.\n",
    "    data_format: channels_last (NHWC) or channels_first (NCHW).\n",
    "    num_layers: number of layers, an int.\n",
    "    batch_norm_decay: decay for batch normalization, a float.\n",
    "    batch_norm_epsilon: epsilon for batch normalization, a float.\n",
    "    Returns:\n",
    "    A tuple with the loss for the tower, the gradients and parameters, and\n",
    "    predictions.\n",
    "    \"\"\"\n",
    "    model = bcl_model.BCL(\n",
    "        batch_norm_decay=batch_norm_decay,\n",
    "        batch_norm_epsilon=batch_norm_epsilon,\n",
    "        is_training=is_training,\n",
    "        data_format=data_format)\n",
    "    #tower_pred = model.forward_pass(inputs, input_data_format='channels_last')\n",
    "    tower_pred = model.forward_pass(inputs)\n",
    "\n",
    "    tower_loss = tf.losses.mean_squared_error(\n",
    "        labels=groundtruths, predictions=tower_pred)\n",
    "    tower_loss = tf.reduce_mean(tower_loss)\n",
    "\n",
    "    model_params = tf.trainable_variables()\n",
    "    #tower_loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in model_params])\n",
    "\n",
    "    tower_grad = tf.gradients(tower_loss, model_params)\n",
    "\n",
    "    return tower_loss, zip(tower_grad, model_params), tower_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_dir,\n",
    "             subset,\n",
    "             num_shards,\n",
    "             batch_size,\n",
    "             use_distortion_for_training=True):\n",
    "    \"\"\"Create input graph for model.\n",
    "        Args:\n",
    "        data_dir: Directory where TFRecords representing the dataset are located.\n",
    "        subset: one of 'train', 'validate' and 'eval'.\n",
    "        num_shards: num of towers participating in data-parallel training.\n",
    "        batch_size: total batch size for training to be divided by the number of\n",
    "        shards.\n",
    "        use_distortion_for_training: True to use distortions.\n",
    "        Returns:\n",
    "        two lists of tensors for features and labels, each of num_shards length.\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        use_distortion = subset == 'train' and use_distortion_for_training\n",
    "        dataset = bcl.BCLDataSet(data_dir, subset, use_distortion)\n",
    "        image_batch, label_batch = dataset.make_batch(batch_size)\n",
    "        if num_shards <= 1:\n",
    "            # No GPU available or only 1 GPU.\n",
    "            return [image_batch], [label_batch]\n",
    "\n",
    "        # Note that passing num=batch_size is safe here, even though\n",
    "        # dataset.batch(batch_size) can, in some cases, return fewer than batch_size\n",
    "        # examples. This is because it does so only when repeating for a limited\n",
    "        # number of epochs, but our dataset repeats forever.\n",
    "        image_batch = tf.unstack(image_batch, num=batch_size, axis=0)\n",
    "        label_batch = tf.unstack(label_batch, num=batch_size, axis=0)\n",
    "        image_shards = [[] for i in range(num_shards)]\n",
    "        label_shards = [[] for i in range(num_shards)]\n",
    "        for i in range(batch_size):\n",
    "            idx = i % num_shards\n",
    "            image_shards[idx].append(image_batch[i])\n",
    "            label_shards[idx].append(label_batch[i])\n",
    "        image_shards = [tf.parallel_stack(x) for x in image_shards]\n",
    "        label_shards = [tf.parallel_stack(x) for x in label_shards]\n",
    "        return image_shards, label_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_fn(data_dir,\n",
    "                      num_gpus,\n",
    "                      variable_strategy,\n",
    "                      use_distortion_for_training=True):\n",
    "    \"\"\"Returns an Experiment function.\n",
    "    Experiments perform training on several workers in parallel,\n",
    "    in other words experiments know how to invoke train and eval in a sensible\n",
    "    fashion for distributed training. Arguments passed directly to this\n",
    "    function are not tunable, all other arguments should be passed within\n",
    "    tf.HParams, passed to the enclosed function.\n",
    "    Args:\n",
    "      data_dir: str. Location of the data for input_fns.\n",
    "      num_gpus: int. Number of GPUs on each worker.\n",
    "      variable_strategy: String. CPU to use CPU as the parameter server\n",
    "      and GPU to use the GPUs as the parameter server.\n",
    "      use_distortion_for_training: bool. See cifar10.Cifar10DataSet.\n",
    "    Returns:\n",
    "      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n",
    "      tf.contrib.learn.Experiment.\n",
    "      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n",
    "      methods on Experiment (train, evaluate) based on information\n",
    "      about the current runner in `run_config`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _experiment_fn(run_config, hparams):\n",
    "        \"\"\"Returns an Experiment.\"\"\"\n",
    "        # Create estimator.\n",
    "        train_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='train',\n",
    "            num_shards=num_gpus,\n",
    "            batch_size=hparams.train_batch_size,\n",
    "            use_distortion_for_training=use_distortion_for_training)\n",
    "\n",
    "        val_input_fn = functools.partial(\n",
    "            input_fn,\n",
    "            data_dir,\n",
    "            subset='val',\n",
    "            batch_size=hparams.val_batch_size,\n",
    "            num_shards=num_gpus)\n",
    "\n",
    "        train_steps = hparams.train_steps\n",
    "\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn=get_model_fn(num_gpus, variable_strategy,\n",
    "                                  run_config.num_worker_replicas or 1),\n",
    "                                  config=run_config,\n",
    "                                  params=hparams)\n",
    "\n",
    "        #train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=train_steps)\n",
    "        #eval_spec = tf.estimator.EvalSpec(input_fn=val_input_fn)\n",
    "        #tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "        # Create experiment.\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            estimator,\n",
    "            train_input_fn=train_input_fn,\n",
    "            eval_input_fn=val_input_fn,\n",
    "            train_steps=train_steps,\n",
    "            eval_steps=100,\n",
    "            min_eval_frequency=10)\n",
    "\n",
    "    return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(job_dir, data_dir, num_gpus, variable_strategy,\n",
    "         use_distortion_for_training, log_device_placement, num_intra_threads,\n",
    "         **hparams):\n",
    "    # The env variable is on deprecation path, default is set to off.\n",
    "    os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "    print('hparams:', hparams)\n",
    "\n",
    "    # Session configuration.\n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=log_device_placement,\n",
    "        intra_op_parallelism_threads=num_intra_threads,\n",
    "        gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n",
    "\n",
    "    config = cifar10_utils.RunConfig(\n",
    "        session_config=sess_config, model_dir=job_dir, \n",
    "        save_checkpoints_steps=hparams[\"eval_steps\"])\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "        get_experiment_fn(data_dir, num_gpus, variable_strategy,\n",
    "                          use_distortion_for_training),\n",
    "        run_config=config,\n",
    "        schedule=\"train_and_evaluate\",\n",
    "        hparams=tf.contrib.training.HParams(\n",
    "            is_chief=config.is_chief,\n",
    "            **hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"sequence_data/12\",\n",
    "    \"job_dir\": \"tmp/bcl\",\n",
    "    \"variable_strategy\": \"CPU\",\n",
    "    \"num_gpus\": len(used_gpus),\n",
    "    \"num_layers\": 4,\n",
    "    \"train_steps\": 15000,\n",
    "    \"eval_steps\": 100,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"val_batch_size\": 128,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 2e-4,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"use_distortion_for_training\": False,\n",
    "    \"sync\": False,\n",
    "    \"num_intra_threads\": 0,\n",
    "    \"num_inter_threads\": 0,\n",
    "    \"data_format\": \"channels_last\",\n",
    "    \"log_device_placement\": False,\n",
    "    \"batch_norm_decay\": 0.997,\n",
    "    \"batch_norm_epsilon\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams: {'num_layers': 4, 'train_steps': 15000, 'eval_steps': 100, 'train_batch_size': 128, 'val_batch_size': 128, 'momentum': 0.9, 'weight_decay': 0.0002, 'learning_rate': 0.001, 'sync': False, 'num_inter_threads': 0, 'data_format': 'channels_last', 'batch_norm_decay': 0.997, 'batch_norm_epsilon': 1e-05}\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7c456fa860>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_save_checkpoints_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'tmp/bcl'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 14400 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:48:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:48:20\n",
      "INFO:tensorflow:Saving dict for global step 14400: global_step = 14400, loss = 0.2993627, mse = 0.2993624\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14400: tmp/bcl/model.ckpt-14400\n",
      "INFO:tensorflow:Validation (step 14401): loss = 0.2993627, mse = 0.2993624, global_step = 14400\n",
      "INFO:tensorflow:loss = 0.16844323, step = 14400\n",
      "INFO:tensorflow:loss = 0.16844323\n",
      "INFO:tensorflow:Average examples/sec: 114.281 (114.281), step = 10\n",
      "INFO:tensorflow:Average examples/sec: 209.726 (1272.45), step = 20\n",
      "INFO:tensorflow:Average examples/sec: 291.615 (1331.1), step = 30\n",
      "INFO:tensorflow:Average examples/sec: 362.591 (1343.74), step = 40\n",
      "INFO:tensorflow:Average examples/sec: 424.179 (1323.16), step = 50\n",
      "INFO:tensorflow:Average examples/sec: 478.438 (1327.46), step = 60\n",
      "INFO:tensorflow:Average examples/sec: 526.819 (1339.6), step = 70\n",
      "INFO:tensorflow:Average examples/sec: 569.428 (1312.54), step = 80\n",
      "INFO:tensorflow:Average examples/sec: 606.807 (1277.85), step = 90\n",
      "INFO:tensorflow:Saving checkpoints for 14500 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 2.55728\n",
      "INFO:tensorflow:loss = 0.17965189, step = 14500 (21.450 sec)\n",
      "INFO:tensorflow:loss = 0.17965189 (21.450 sec)\n",
      "INFO:tensorflow:Average examples/sec: 596.723 (519.09), step = 100\n",
      "INFO:tensorflow:Average examples/sec: 627.697 (1305.14), step = 110\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:48:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:49:01\n",
      "INFO:tensorflow:Saving dict for global step 14500: global_step = 14500, loss = 0.30836406, mse = 0.3083639\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14500: tmp/bcl/model.ckpt-14500\n",
      "INFO:tensorflow:Validation (step 14511): loss = 0.30836406, mse = 0.3083639, global_step = 14500\n",
      "INFO:tensorflow:Average examples/sec: 369.924 (67.048), step = 120\n",
      "INFO:tensorflow:Average examples/sec: 391.321 (1279.18), step = 130\n",
      "INFO:tensorflow:Average examples/sec: 411.821 (1291.16), step = 140\n",
      "INFO:tensorflow:Average examples/sec: 431.481 (1301.03), step = 150\n",
      "INFO:tensorflow:Average examples/sec: 450.029 (1266.91), step = 160\n",
      "INFO:tensorflow:Average examples/sec: 467.923 (1286.16), step = 170\n",
      "INFO:tensorflow:Average examples/sec: 485.154 (1297.33), step = 180\n",
      "INFO:tensorflow:Average examples/sec: 501.814 (1314.04), step = 190\n",
      "INFO:tensorflow:Saving checkpoints for 14600 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.38637\n",
      "INFO:tensorflow:loss = 0.17018479, step = 14600 (29.531 sec)\n",
      "INFO:tensorflow:loss = 0.17018479 (29.531 sec)\n",
      "INFO:tensorflow:Average examples/sec: 502.139 (508.378), step = 200\n",
      "INFO:tensorflow:Average examples/sec: 517.074 (1276.28), step = 210\n",
      "INFO:tensorflow:Average examples/sec: 531.524 (1286.57), step = 220\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:49:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:49:31\n",
      "INFO:tensorflow:Saving dict for global step 14600: global_step = 14600, loss = 0.28847533, mse = 0.2884752\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14600: tmp/bcl/model.ckpt-14600\n",
      "INFO:tensorflow:Validation (step 14621): loss = 0.28847533, mse = 0.2884752, global_step = 14600\n",
      "INFO:tensorflow:Average examples/sec: 407.708 (66.5667), step = 230\n",
      "INFO:tensorflow:Average examples/sec: 419.602 (1275.33), step = 240\n",
      "INFO:tensorflow:Average examples/sec: 431.275 (1297.68), step = 250\n",
      "INFO:tensorflow:Average examples/sec: 442.525 (1272.24), step = 260\n",
      "INFO:tensorflow:Average examples/sec: 453.487 (1273.92), step = 270\n",
      "INFO:tensorflow:Average examples/sec: 464.288 (1300.87), step = 280\n",
      "INFO:tensorflow:Average examples/sec: 474.845 (1307.01), step = 290\n",
      "INFO:tensorflow:Saving checkpoints for 14700 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.36591\n",
      "INFO:tensorflow:loss = 0.17330882, step = 14700 (29.711 sec)\n",
      "INFO:tensorflow:loss = 0.17330882 (29.711 sec)\n",
      "INFO:tensorflow:Average examples/sec: 475.881 (508.021), step = 300\n",
      "INFO:tensorflow:Average examples/sec: 485.693 (1273.14), step = 310\n",
      "INFO:tensorflow:Average examples/sec: 495.339 (1288.87), step = 320\n",
      "INFO:tensorflow:Average examples/sec: 504.848 (1308.96), step = 330\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:49:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14700\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:50:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 14700: global_step = 14700, loss = 0.28622422, mse = 0.2862241\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14700: tmp/bcl/model.ckpt-14700\n",
      "INFO:tensorflow:Validation (step 14731): loss = 0.28622422, mse = 0.2862241, global_step = 14700\n",
      "INFO:tensorflow:Average examples/sec: 423.076 (66.6777), step = 340\n",
      "INFO:tensorflow:Average examples/sec: 431.341 (1284.42), step = 350\n",
      "INFO:tensorflow:Average examples/sec: 439.543 (1314.16), step = 360\n",
      "INFO:tensorflow:Average examples/sec: 447.646 (1330.84), step = 370\n",
      "INFO:tensorflow:Average examples/sec: 455.582 (1324.08), step = 380\n",
      "INFO:tensorflow:Average examples/sec: 463.4 (1332.24), step = 390\n",
      "INFO:tensorflow:Saving checkpoints for 14800 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.40072\n",
      "INFO:tensorflow:loss = 0.16879925, step = 14800 (29.404 sec)\n",
      "INFO:tensorflow:loss = 0.16879925 (29.404 sec)\n",
      "INFO:tensorflow:Average examples/sec: 465.048 (539.901), step = 400\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:50:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14800\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:50:28\n",
      "INFO:tensorflow:Saving dict for global step 14800: global_step = 14800, loss = 0.30483562, mse = 0.3048355\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14800: tmp/bcl/model.ckpt-14800\n",
      "INFO:tensorflow:Validation (step 14801): loss = 0.30483562, mse = 0.3048355, global_step = 14800\n",
      "INFO:tensorflow:Average examples/sec: 407.283 (68.2383), step = 410\n",
      "INFO:tensorflow:Average examples/sec: 414.021 (1287.13), step = 420\n",
      "INFO:tensorflow:Average examples/sec: 420.701 (1305.04), step = 430\n",
      "INFO:tensorflow:Average examples/sec: 427.222 (1281.18), step = 440\n",
      "INFO:tensorflow:Average examples/sec: 433.635 (1277.19), step = 450\n",
      "INFO:tensorflow:Average examples/sec: 440.014 (1301.62), step = 460\n",
      "INFO:tensorflow:Average examples/sec: 446.361 (1326.76), step = 470\n",
      "INFO:tensorflow:Average examples/sec: 452.621 (1328), step = 480\n",
      "INFO:tensorflow:Average examples/sec: 458.796 (1329.09), step = 490\n",
      "INFO:tensorflow:Saving checkpoints for 14900 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 3.43741\n",
      "INFO:tensorflow:loss = 0.17779905, step = 14900 (29.091 sec)\n",
      "INFO:tensorflow:loss = 0.17779905 (29.091 sec)\n",
      "INFO:tensorflow:Average examples/sec: 459.811 (515.721), step = 500\n",
      "INFO:tensorflow:Average examples/sec: 465.659 (1278.93), step = 510\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:50:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-14900\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:50:59\n",
      "INFO:tensorflow:Saving dict for global step 14900: global_step = 14900, loss = 0.33834076, mse = 0.3383405\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14900: tmp/bcl/model.ckpt-14900\n",
      "INFO:tensorflow:Validation (step 14911): loss = 0.33834076, mse = 0.3383405, global_step = 14900\n",
      "INFO:tensorflow:Average examples/sec: 416.539 (65.2911), step = 520\n",
      "INFO:tensorflow:Average examples/sec: 421.909 (1280.2), step = 530\n",
      "INFO:tensorflow:Average examples/sec: 427.257 (1301.9), step = 540\n",
      "INFO:tensorflow:Average examples/sec: 432.593 (1328.27), step = 550\n",
      "INFO:tensorflow:Average examples/sec: 437.844 (1317.33), step = 560\n",
      "INFO:tensorflow:Average examples/sec: 442.95 (1276.86), step = 570\n",
      "INFO:tensorflow:Average examples/sec: 448.068 (1312.22), step = 580\n",
      "INFO:tensorflow:Average examples/sec: 453.123 (1310.77), step = 590\n",
      "INFO:tensorflow:Saving checkpoints for 15000 into tmp/bcl/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.16943978.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-16-10:51:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/bcl/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-16-10:51:27\n",
      "INFO:tensorflow:Saving dict for global step 15000: global_step = 15000, loss = 0.30180103, mse = 0.3018009\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15000: tmp/bcl/model.ckpt-15000\n"
     ]
    }
   ],
   "source": [
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
