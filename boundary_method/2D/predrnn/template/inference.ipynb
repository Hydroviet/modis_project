{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from skimage.measure import compare_ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import predrnn_pp\n",
    "import dataset_utils\n",
    "from nets import models_factory\n",
    "from data_provider import datasets_factory\n",
    "from utils import preprocess\n",
    "from utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modis_utils.misc import cache_data, restore_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../one_output/data_patch'\n",
    "model_name = 'predrnn_pp'\n",
    "save_dir = 'results/predrnn_pp'\n",
    "input_length = 14\n",
    "output_length = 1\n",
    "img_width = 32\n",
    "img_channel = 1\n",
    "stride = 1\n",
    "filter_size = 5\n",
    "num_hidden = [128, 64, 64, 1]\n",
    "num_layers = len(num_hidden)\n",
    "patch_size = 4\n",
    "layer_norm = True\n",
    "lr = 0.001\n",
    "reverse_input = False\n",
    "batch_size = 8\n",
    "max_iterations = 80000\n",
    "display_interval = 1\n",
    "test_interval = 2000\n",
    "snapshot_interval = 10000\n",
    "\n",
    "save_checkpoints_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"data_dir\" : data_dir,\n",
    "    \"model_name\" :  model_name,\n",
    "    \"save_dir\" : save_dir,\n",
    "    \"input_length\" : input_length,\n",
    "    \"output_length\" : output_length,\n",
    "    \"seq_length\" : input_length + output_length,\n",
    "    \"img_width\" : img_width,\n",
    "    \"img_channel\" : img_channel,\n",
    "    \"stride\" : stride,\n",
    "    \"filter_size\" : filter_size,\n",
    "    \"num_hidden\" : num_hidden,\n",
    "    \"num_layers\" : num_layers,\n",
    "    \"patch_size\" : patch_size,\n",
    "    \"layer_norm\" : layer_norm,\n",
    "    \"lr\" : lr,\n",
    "    \"reverse_input\" : reverse_input,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"max_iterations\" : max_iterations,\n",
    "    \"display_interval\" : display_interval,\n",
    "    \"test_interval\" : test_interval,\n",
    "    \"snapshot_interval\" : snapshot_interval\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    predictions = predrnn_pp.rnn(features, params[\"num_layers\"], params[\"num_hidden\"], params[\"filter_size\"],\n",
    "                                 params[\"stride\"], params[\"seq_length\"], params[\"input_length\"], \n",
    "                                 params[\"layer_norm\"])\n",
    "    predictions = predictions[:, params[\"input_length\"]-1:]\n",
    "    print(\"predictions.shape =\", predictions.shape)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    else:\n",
    "        loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n",
    "        tensors_to_log = {'loss': loss}\n",
    "        logging_hook = tf.train.LoggingTensorHook(\n",
    "            tensors=tensors_to_log, every_n_iter=100)\n",
    "        train_hooks = [logging_hook]\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': tf.metrics.mean_squared_error(labels=labels, predictions=predictions)\n",
    "        }\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=params[\"lr\"])\\\n",
    "                .minimize(loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_dir, subset, batch_size,\n",
    "             use_distortion_for_training=True):\n",
    "    use_distortion = subset == 'train' and use_distortion_for_training\n",
    "    dataset = dataset_utils.ConvLSTMDataSet(data_dir, subset, use_distortion)\n",
    "    return dataset.make_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'results/predrnn_pp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f36b42a50f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "test_inpf = functools.partial(input_fn, data_dir, 'test', batch_size)\n",
    "val_inpf = functools.partial(input_fn, data_dir, 'val', batch_size)\n",
    "train_inpf = functools.partial(input_fn, data_dir, 'train', batch_size)\n",
    "\n",
    "cfg = tf.estimator.RunConfig(save_checkpoints_steps=save_checkpoints_steps)\n",
    "estimator = tf.estimator.Estimator(model_fn, save_dir, cfg, params)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "predictions.shape = (8, 1, 32, 32, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-28-09:31:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/predrnn_pp/model.ckpt-21300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [17/172]\n",
      "INFO:tensorflow:Evaluation [34/172]\n",
      "INFO:tensorflow:Evaluation [51/172]\n",
      "INFO:tensorflow:Evaluation [68/172]\n",
      "INFO:tensorflow:Evaluation [85/172]\n",
      "INFO:tensorflow:Evaluation [102/172]\n",
      "INFO:tensorflow:Evaluation [119/172]\n",
      "INFO:tensorflow:Evaluation [136/172]\n",
      "INFO:tensorflow:Evaluation [153/172]\n",
      "INFO:tensorflow:Evaluation [170/172]\n",
      "INFO:tensorflow:Evaluation [172/172]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-28-09:32:54\n",
      "INFO:tensorflow:Saving dict for global step 21300: global_step = 21300, loss = 0.012188795, mse = 0.012188795\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21300: results/predrnn_pp/model.ckpt-21300\n"
     ]
    }
   ],
   "source": [
    "result_test = estimator.evaluate(test_inpf,\n",
    "    steps=dataset_utils.ConvLSTMDataSet.num_examples_per_epoch('test')//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.012188795, 'mse': 0.012188795, 'global_step': 21300}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "predictions.shape = (8, 1, 32, 32, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/predrnn_pp/model.ckpt-21300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "results = estimator.predict(test_inpf)\n",
    "inferences = []\n",
    "i = 0\n",
    "for result in results:\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    inferences.append(result)\n",
    "    i += 1\n",
    "    if i == dataset_utils.ConvLSTMDataSet.num_examples_per_epoch('test'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = np.vstack(inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('inferences'):\n",
    "    os.makedirs('inferences')\n",
    "cache_data(inferences, 'inferences/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2786, 32, 32, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = restore_data('inferences/test.dat')\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "predictions.shape = (8, 1, 32, 32, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-28-09:35:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/predrnn_pp/model.ckpt-21300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [17/172]\n",
      "INFO:tensorflow:Evaluation [34/172]\n",
      "INFO:tensorflow:Evaluation [51/172]\n",
      "INFO:tensorflow:Evaluation [68/172]\n",
      "INFO:tensorflow:Evaluation [85/172]\n",
      "INFO:tensorflow:Evaluation [102/172]\n",
      "INFO:tensorflow:Evaluation [119/172]\n",
      "INFO:tensorflow:Evaluation [136/172]\n",
      "INFO:tensorflow:Evaluation [153/172]\n",
      "INFO:tensorflow:Evaluation [170/172]\n",
      "INFO:tensorflow:Evaluation [172/172]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-28-09:36:45\n",
      "INFO:tensorflow:Saving dict for global step 21300: global_step = 21300, loss = 0.009067808, mse = 0.009067808\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21300: results/predrnn_pp/model.ckpt-21300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.009067808, 'mse': 0.009067808, 'global_step': 21300}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val = estimator.evaluate(val_inpf,\n",
    "    steps=dataset_utils.ConvLSTMDataSet.num_examples_per_epoch('val')//batch_size)\n",
    "result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle dataset\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "predictions.shape = (8, 1, 32, 32, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-28-09:36:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/predrnn_pp/model.ckpt-21300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [201/2012]\n",
      "INFO:tensorflow:Evaluation [402/2012]\n",
      "INFO:tensorflow:Evaluation [603/2012]\n",
      "INFO:tensorflow:Evaluation [804/2012]\n",
      "INFO:tensorflow:Evaluation [1005/2012]\n",
      "INFO:tensorflow:Evaluation [1206/2012]\n",
      "INFO:tensorflow:Evaluation [1407/2012]\n",
      "INFO:tensorflow:Evaluation [1608/2012]\n",
      "INFO:tensorflow:Evaluation [1809/2012]\n",
      "INFO:tensorflow:Evaluation [2010/2012]\n",
      "INFO:tensorflow:Evaluation [2012/2012]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-28-09:49:47\n",
      "INFO:tensorflow:Saving dict for global step 21300: global_step = 21300, loss = 0.010196711, mse = 0.010196711\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21300: results/predrnn_pp/model.ckpt-21300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.010196711, 'mse': 0.010196711, 'global_step': 21300}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_train = estimator.evaluate(train_inpf,\n",
    "    steps=dataset_utils.ConvLSTMDataSet.num_examples_per_epoch('train')//batch_size)\n",
    "result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inferences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2786"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34e379b3c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 14, 32, 32), (30, 1, 32, 32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_path = '../one_output/sequence_patch_data/test/0.dat'\n",
    "sample_test = restore_data(sample_test_path)\n",
    "sample_inputs = sample_test[0]\n",
    "sample_labels = sample_test[1]\n",
    "sample_inputs.shape, sample_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def convert_to_tfrecord(inputs):\n",
    "    r = np.random.randint(1000000)\n",
    "    path = '.{}'.format(r)\n",
    "    with tf.python_io.TFRecordWriter(path) as record_writer:\n",
    "        num_entries_in_batch = len(inputs)\n",
    "        for i in range(num_entries_in_batch):\n",
    "            example = tf.train.Example(features=tf.train.Features(\n",
    "                feature={\n",
    "                    'inputs': _float_feature(inputs[i].flatten().tolist()),\n",
    "                }))\n",
    "            record_writer.write(example.SerializeToString())\n",
    "    return path\n",
    "        \n",
    "def parser(serialized_example, single_example_shape):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'inputs': tf.FixedLenFeature(\n",
    "                [single_example_shape[0] * single_example_shape[1] * single_example_shape[2]],\n",
    "                tf.float32),\n",
    "        })\n",
    "    inputs = tf.reshape(features['inputs'], single_example_shape)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_from_np_array(estimator, inputs_np):\n",
    "    tfrecord_path = convert_to_tfrecord(inputs_np)\n",
    "    shape = inputs_np.shape\n",
    "    single_example_shape = shape[1:] + (1,)\n",
    "    def np_input_fn():\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        dataset = dataset.map(\n",
    "            functools.partial(\n",
    "                parser, \n",
    "                single_example_shape=single_example_shape))\n",
    "        dataset = dataset.batch(1, drop_remainder=True)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        inputs = iterator.get_next()\n",
    "        return inputs\n",
    "    \n",
    "    result = estimator.predict(np_input_fn)\n",
    "    res1 = []\n",
    "    for res in result:\n",
    "        res1.append(res)\n",
    "    os.remove(tfrecord_path)\n",
    "    return np.vstack(res1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "predictions.shape = (1, 1, 32, 32, 1)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from results/predrnn_pp/model.ckpt-21300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "res = inference_from_np_array(estimator, sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 32, 32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
